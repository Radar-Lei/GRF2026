Peng Ding

# A First Course in Causal Inference

arXiv:2305.18793v2 [stat.ME] 3 Oct 2023![[_resources/Ding - 2023 - A First Course in Causal Inference_Part1/3524a9419254cbda4a4ca7c2e075654d_MD5.jpg]]

![[_resources/Ding - 2023 - A First Course in Causal Inference_Part1/17af20a572b5c03aa521f59df96d9e97_MD5.jpg]]

![[_resources/Ding - 2023 - A First Course in Causal Inference_Part1/5a01de1bca90f7846b78b70b9d226ad4_MD5.jpg]]

![[_resources/Ding - 2023 - A First Course in Causal Inference_Part1/f09e0367d325873e08956b87be368fee_MD5.jpg]]*To students and readers who are interested in causal inference*—Contents

<table>
  <tbody>
    <tr>
      <td>Preface</td>
      <td>xvii</td>
    </tr>
    <tr>
      <td>Acronyms</td>
      <td>xxiii</td>
    </tr>
    <tr>
      <td>Notation</td>
      <td>xxv</td>
    </tr>
    <tr>
      <td><b>I Introduction</b></td>
      <td><b>1</b></td>
    </tr>
    <tr>
      <td><b>1 Correlation, Association, and the Yule-Simpson Paradox</b></td>
      <td><b>3</b></td>
    </tr>
    <tr>
      <td>1.1 Traditional view of statistics</td>
      <td>3</td>
    </tr>
    <tr>
      <td>1.2 Some commonly-used measures of association</td>
      <td>4</td>
    </tr>
    <tr>
      <td>1.2.1 Correlation and regression</td>
      <td>4</td>
    </tr>
    <tr>
      <td>1.2.2 Contingency tables</td>
      <td>5</td>
    </tr>
    <tr>
      <td>1.3 An example of the Yule-Simpson Paradox</td>
      <td>7</td>
    </tr>
    <tr>
      <td>1.3.1 Data</td>
      <td>7</td>
    </tr>
    <tr>
      <td>1.3.2 Explanation</td>
      <td>8</td>
    </tr>
    <tr>
      <td>1.3.3 Geometry of the Yule-Simpson Paradox</td>
      <td>9</td>
    </tr>
    <tr>
      <td>1.4 The Berkeley graduate school admission data</td>
      <td>10</td>
    </tr>
    <tr>
      <td>1.5 Homework Problems</td>
      <td>13</td>
    </tr>
    <tr>
      <td><b>2 Potential Outcomes</b></td>
      <td><b>15</b></td>
    </tr>
    <tr>
      <td>2.1 Experimentalists' view of causal inference</td>
      <td>15</td>
    </tr>
    <tr>
      <td>2.2 Formal notation of potential outcomes</td>
      <td>16</td>
    </tr>
    <tr>
      <td>2.2.1 Causal effects, subgroups, and the non-existence of Yule-Simpson Paradox</td>
      <td>18</td>
    </tr>
    <tr>
      <td>2.2.2 Subtlety of the definition of the experimental unit</td>
      <td>18</td>
    </tr>
    <tr>
      <td>2.3 Treatment assignment mechanism</td>
      <td>19</td>
    </tr>
    <tr>
      <td>2.4 Homework Problems</td>
      <td>21</td>
    </tr>
    <tr>
      <td><b>II Randomized experiments</b></td>
      <td><b>23</b></td>
    </tr>
    <tr>
      <td><b>3 The Completely Randomized Experiment and the Fisher Randomization Test</b></td>
      <td><b>25</b></td>
    </tr>
    <tr>
      <td>3.1 CRE</td>
      <td>25</td>
    </tr>
    <tr>
      <td>3.2 FRT</td>
      <td>26</td>
    </tr>
    <tr>
      <td>3.3 Canonical choices of the test statistic</td>
      <td>28</td>
    </tr>
    <tr>
      <td>3.4 A case study of the LaLonde experimental data</td>
      <td>33</td>
    </tr>
  </tbody>
</table><table>
  <tbody>
    <tr>
      <td>3.5</td>
      <td>Some history of randomized experiments and FRT</td>
      <td>35</td>
    </tr>
    <tr>
      <td>3.5.1</td>
      <td>James Lind's experiment</td>
      <td>35</td>
    </tr>
    <tr>
      <td>3.5.2</td>
      <td>Lady tasting tea</td>
      <td>37</td>
    </tr>
    <tr>
      <td>3.5.3</td>
      <td>Two Fisherian principles for experiments</td>
      <td>38</td>
    </tr>
    <tr>
      <td>3.6</td>
      <td>Discussion</td>
      <td>38</td>
    </tr>
    <tr>
      <td>3.6.1</td>
      <td>Other sharp null hypotheses and confidence intervals</td>
      <td>38</td>
    </tr>
    <tr>
      <td>3.6.2</td>
      <td>Other test statistics</td>
      <td>39</td>
    </tr>
    <tr>
      <td>3.6.3</td>
      <td>Final remarks</td>
      <td>39</td>
    </tr>
    <tr>
      <td>3.7</td>
      <td>Homework Problems</td>
      <td>40</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Neymanian Repeated Sampling Inference in Completely Randomized Experiments</td>
      <td>43</td>
    </tr>
    <tr>
      <td>4.1</td>
      <td>Finite population quantities</td>
      <td>43</td>
    </tr>
    <tr>
      <td>4.2</td>
      <td>Neyman (1923)'s theorem</td>
      <td>44</td>
    </tr>
    <tr>
      <td>4.3</td>
      <td>Proofs</td>
      <td>47</td>
    </tr>
    <tr>
      <td>4.4</td>
      <td>Regression analysis of the CRE</td>
      <td>49</td>
    </tr>
    <tr>
      <td>4.5</td>
      <td>Examples</td>
      <td>50</td>
    </tr>
    <tr>
      <td>4.5.1</td>
      <td>Simulation</td>
      <td>50</td>
    </tr>
    <tr>
      <td>4.5.2</td>
      <td>Heavy-tailed outcome and failure of Normal approximations</td>
      <td>51</td>
    </tr>
    <tr>
      <td>4.5.3</td>
      <td>Application</td>
      <td>51</td>
    </tr>
    <tr>
      <td>4.6</td>
      <td>Homework Problems</td>
      <td>55</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Stratification and Post-Stratification in Randomized Experiments</td>
      <td>59</td>
    </tr>
    <tr>
      <td>5.1</td>
      <td>Stratification</td>
      <td>59</td>
    </tr>
    <tr>
      <td>5.2</td>
      <td>FRT</td>
      <td>61</td>
    </tr>
    <tr>
      <td>5.2.1</td>
      <td>Theory</td>
      <td>61</td>
    </tr>
    <tr>
      <td>5.2.2</td>
      <td>An application</td>
      <td>63</td>
    </tr>
    <tr>
      <td>5.3</td>
      <td>Neymanian inference</td>
      <td>66</td>
    </tr>
    <tr>
      <td>5.3.1</td>
      <td>Point and interval estimation</td>
      <td>66</td>
    </tr>
    <tr>
      <td>5.3.2</td>
      <td>Numerical examples</td>
      <td>66</td>
    </tr>
    <tr>
      <td>5.3.3</td>
      <td>Comparing the SRE and the CRE</td>
      <td>70</td>
    </tr>
    <tr>
      <td>5.4</td>
      <td>Post-stratification in a CRE</td>
      <td>72</td>
    </tr>
    <tr>
      <td>5.4.1</td>
      <td>Meinert et al. (1970)'s Example</td>
      <td>73</td>
    </tr>
    <tr>
      <td>5.4.2</td>
      <td>Chong et al. (2016)'s Example</td>
      <td>73</td>
    </tr>
    <tr>
      <td>5.5</td>
      <td>Practical questions</td>
      <td>75</td>
    </tr>
    <tr>
      <td>5.6</td>
      <td>Homework Problems</td>
      <td>76</td>
    </tr>
    <tr>
      <td>6</td>
      <td>Rerandomization and Regression Adjustment</td>
      <td>81</td>
    </tr>
    <tr>
      <td>6.1</td>
      <td>Rerandomization</td>
      <td>81</td>
    </tr>
    <tr>
      <td>6.1.1</td>
      <td>Experimental design</td>
      <td>81</td>
    </tr>
    <tr>
      <td>6.1.2</td>
      <td>Statistical inference</td>
      <td>82</td>
    </tr>
    <tr>
      <td>6.2</td>
      <td>Regression adjustment</td>
      <td>85</td>
    </tr>
    <tr>
      <td>6.2.1</td>
      <td>Covariate-adjusted FRT</td>
      <td>85</td>
    </tr>
  </tbody>
</table><table>
  <tbody>
    <tr>
      <td>Contents</td>
      <td>vii</td>
    </tr>
    <tr>
      <td>6.2.2 Analysis of covariance and extensions</td>
      <td>86</td>
    </tr>
    <tr>
      <td>6.2.2.1 Some heuristics for Lin (2013)'s results</td>
      <td>86</td>
    </tr>
    <tr>
      <td>6.2.2.2 Understanding Lin (2013)'s estimator via predicting the potential outcomes</td>
      <td>89</td>
    </tr>
    <tr>
      <td>6.2.2.3 Understanding Lin (2013)'s estimator via adjusting for covariate imbalance</td>
      <td>90</td>
    </tr>
    <tr>
      <td>6.2.3 Some additional remarks on regression adjustment</td>
      <td>90</td>
    </tr>
    <tr>
      <td>6.2.3.1 Duality between ReM and regression adjustment</td>
      <td>90</td>
    </tr>
    <tr>
      <td>6.2.3.2 Equivalence of regression adjustment and post-stratification</td>
      <td>91</td>
    </tr>
    <tr>
      <td>6.2.3.3 Difference-in-difference as a special case of covariate adjustment &tau;&#770;(&beta;<sub>1</sub>, &beta;<sub>0</sub>)</td>
      <td>91</td>
    </tr>
    <tr>
      <td>6.2.4 Extension to the SRE</td>
      <td>92</td>
    </tr>
    <tr>
      <td>6.3 Unification, combination, and comparison</td>
      <td>92</td>
    </tr>
    <tr>
      <td>6.4 Simulation</td>
      <td>93</td>
    </tr>
    <tr>
      <td>6.5 Final remarks</td>
      <td>95</td>
    </tr>
    <tr>
      <td>6.6 Homework Problems</td>
      <td>96</td>
    </tr>
    <tr>
      <td><b>7 Matched-Pairs Experiment</b></td>
      <td><b>99</b></td>
    </tr>
    <tr>
      <td>7.1 Design of the experiment and potential outcomes</td>
      <td>99</td>
    </tr>
    <tr>
      <td>7.2 FRT</td>
      <td>100</td>
    </tr>
    <tr>
      <td>7.3 Neymanian inference</td>
      <td>103</td>
    </tr>
    <tr>
      <td>7.4 Covariate adjustment</td>
      <td>105</td>
    </tr>
    <tr>
      <td>7.4.1 FRT</td>
      <td>106</td>
    </tr>
    <tr>
      <td>7.4.2 Regression adjustment</td>
      <td>106</td>
    </tr>
    <tr>
      <td>7.5 Examples</td>
      <td>108</td>
    </tr>
    <tr>
      <td>7.5.1 Darwin's data comparing cross-fertilizing and self-fertilizing on the height of corns</td>
      <td>108</td>
    </tr>
    <tr>
      <td>7.5.2 Children's television workshop experiment data</td>
      <td>110</td>
    </tr>
    <tr>
      <td>7.6 Comparing the MPE and CRE</td>
      <td>112</td>
    </tr>
    <tr>
      <td>7.7 Extension to the general matched experiment</td>
      <td>113</td>
    </tr>
    <tr>
      <td>7.7.1 FRT</td>
      <td>113</td>
    </tr>
    <tr>
      <td>7.7.2 Estimating the average of the within-strata effects</td>
      <td>114</td>
    </tr>
    <tr>
      <td>7.7.3 A more general causal estimand</td>
      <td>114</td>
    </tr>
    <tr>
      <td>7.8 Homework Problems</td>
      <td>115</td>
    </tr>
    <tr>
      <td><b>8 Unification of the Fisherian and Neymanian Inferences in Randomized Experiments</b></td>
      <td><b>119</b></td>
    </tr>
    <tr>
      <td>8.1 Testing strong and weak null hypotheses in the CRE</td>
      <td>119</td>
    </tr>
    <tr>
      <td>8.2 Covariate-adjusted FRTs in the CRE</td>
      <td>121</td>
    </tr>
    <tr>
      <td>8.3 A simulation study</td>
      <td>122</td>
    </tr>
    <tr>
      <td>8.4 General recommendations</td>
      <td>122</td>
    </tr>
    <tr>
      <td>8.5 A case study</td>
      <td>124</td>
    </tr>
    <tr>
      <td>8.6 Homework Problems</td>
      <td>125</td>
    </tr>
  </tbody>
</table><table>
  <tbody>
    <tr>
      <td><b>9</b></td>
      <td><b>Bridging Finite and Super Population Causal Inference</b></td>
      <td><b>129</b></td>
    </tr>
    <tr>
      <td>9.1</td>
      <td>CRE</td>
      <td>130</td>
    </tr>
    <tr>
      <td>9.2</td>
      <td>Simulation under the CRE: the super population perspective</td>
      <td>132</td>
    </tr>
    <tr>
      <td>9.3</td>
      <td>Extension to the SRE</td>
      <td>133</td>
    </tr>
    <tr>
      <td>9.4</td>
      <td>Homework Problems</td>
      <td>134</td>
    </tr>
    <tr>
      <td><b>III</b></td>
      <td><b>Observational studies</b></td>
      <td><b>137</b></td>
    </tr>
    <tr>
      <td><b>10</b></td>
      <td><b>Observational Studies, Selection Bias, and Nonparametric Identification of Causal Effects</b></td>
      <td><b>139</b></td>
    </tr>
    <tr>
      <td>10.1</td>
      <td>Motivating Examples</td>
      <td>139</td>
    </tr>
    <tr>
      <td>10.2</td>
      <td>Causal effects and selection bias under the potential outcomes framework</td>
      <td>141</td>
    </tr>
    <tr>
      <td>10.3</td>
      <td>Sufficient conditions for nonparametric identification of causal effects</td>
      <td>142</td>
    </tr>
    <tr>
      <td>10.3.1</td>
      <td>Identification</td>
      <td>142</td>
    </tr>
    <tr>
      <td>10.3.2</td>
      <td>Plausibility of the ignorability assumption</td>
      <td>145</td>
    </tr>
    <tr>
      <td>10.4</td>
      <td>Two simple estimation strategies and their limitations</td>
      <td>146</td>
    </tr>
    <tr>
      <td>10.4.1</td>
      <td>Stratification or standardization based on discrete covariates</td>
      <td>146</td>
    </tr>
    <tr>
      <td>10.4.2</td>
      <td>Outcome regression</td>
      <td>147</td>
    </tr>
    <tr>
      <td>10.5</td>
      <td>Homework Problems</td>
      <td>149</td>
    </tr>
    <tr>
      <td><b>11</b></td>
      <td><b>The Central Role of the Propensity Score in Observational Studies for Causal Effects</b></td>
      <td><b>153</b></td>
    </tr>
    <tr>
      <td>11.1</td>
      <td>The propensity score as a dimension reduction tool</td>
      <td>154</td>
    </tr>
    <tr>
      <td>11.1.1</td>
      <td>Theory</td>
      <td>154</td>
    </tr>
    <tr>
      <td>11.1.2</td>
      <td>Propensity score stratification</td>
      <td>155</td>
    </tr>
    <tr>
      <td>11.1.3</td>
      <td>Application</td>
      <td>157</td>
    </tr>
    <tr>
      <td>11.2</td>
      <td>Propensity score weighting</td>
      <td>158</td>
    </tr>
    <tr>
      <td>11.2.1</td>
      <td>Theory</td>
      <td>158</td>
    </tr>
    <tr>
      <td>11.2.2</td>
      <td>Inverse propensity score weighting estimators</td>
      <td>160</td>
    </tr>
    <tr>
      <td>11.2.3</td>
      <td>A problem of IPW and a fundamental problem of causal inference</td>
      <td>160</td>
    </tr>
    <tr>
      <td>11.2.4</td>
      <td>Application</td>
      <td>161</td>
    </tr>
    <tr>
      <td>11.3</td>
      <td>The balancing property of the propensity score</td>
      <td>163</td>
    </tr>
    <tr>
      <td>11.3.1</td>
      <td>Theory</td>
      <td>163</td>
    </tr>
    <tr>
      <td>11.3.2</td>
      <td>Covariate balance check</td>
      <td>164</td>
    </tr>
    <tr>
      <td>11.4</td>
      <td>Homework Problems</td>
      <td>164</td>
    </tr>
    <tr>
      <td><b>12</b></td>
      <td><b>The Doubly Robust or the Augmented Inverse Propensity Score Weighting Estimator for the Average Causal Effect</b></td>
      <td><b>169</b></td>
    </tr>
    <tr>
      <td>12.1</td>
      <td>The doubly robust estimator</td>
      <td>170</td>
    </tr>
    <tr>
      <td>12.1.1</td>
      <td>Population version</td>
      <td>170</td>
    </tr>
    <tr>
      <td>12.1.2</td>
      <td>Sample version</td>
      <td>171</td>
    </tr>
    <tr>
      <td>12.2</td>
      <td>More intuition and theory for the doubly robust estimator</td>
      <td>172</td>
    </tr>
  </tbody>
</table><table>
  <tbody>
    <tr>
      <td>Contents</td>
      <td>ix</td>
    </tr>
    <tr>
      <td>12.2.1 Reducing the variance of the IPW estimator</td>
      <td>172</td>
    </tr>
    <tr>
      <td>12.2.2 Reducing the bias of the outcome regression estimator</td>
      <td>173</td>
    </tr>
    <tr>
      <td>12.3 Examples</td>
      <td>173</td>
    </tr>
    <tr>
      <td>12.3.1 Summary of some canonical estimators for &tau;</td>
      <td>173</td>
    </tr>
    <tr>
      <td>12.3.2 Simulation</td>
      <td>175</td>
    </tr>
    <tr>
      <td>12.3.3 Applications</td>
      <td>176</td>
    </tr>
    <tr>
      <td>12.4 Some further discussion</td>
      <td>177</td>
    </tr>
    <tr>
      <td>12.5 Homework problems</td>
      <td>178</td>
    </tr>
    <tr>
      <td><b>13 The Average Causal Effect on the Treated Units and Other<br>Estimands</b></td>
      <td><b>181</b></td>
    </tr>
    <tr>
      <td>13.1 Nonparametric identification of &tau;<sub>T</sub></td>
      <td>181</td>
    </tr>
    <tr>
      <td>13.2 Inverse propensity score weighting and doubly robust estima-<br>tion of &tau;<sub>T</sub></td>
      <td>183</td>
    </tr>
    <tr>
      <td>13.3 An example</td>
      <td>186</td>
    </tr>
    <tr>
      <td>13.4 Other estimands</td>
      <td>187</td>
    </tr>
    <tr>
      <td>13.5 Homework Problems</td>
      <td>189</td>
    </tr>
    <tr>
      <td><b>14 Using the Propensity Score in Regressions for Causal Effects</b></td>
      <td><b>193</b></td>
    </tr>
    <tr>
      <td>14.1 Regressions with the propensity score as a covariate</td>
      <td>193</td>
    </tr>
    <tr>
      <td>14.2 Regressions weighted by the inverse of the propensity score</td>
      <td>196</td>
    </tr>
    <tr>
      <td>14.2.1 Average causal effect</td>
      <td>196</td>
    </tr>
    <tr>
      <td>14.2.2 Average causal effect on the treated units</td>
      <td>199</td>
    </tr>
    <tr>
      <td>14.3 Homework problems</td>
      <td>200</td>
    </tr>
    <tr>
      <td><b>15 Matching in Observational Studies</b></td>
      <td><b>203</b></td>
    </tr>
    <tr>
      <td>15.1 A simple starting point: many more control units</td>
      <td>203</td>
    </tr>
    <tr>
      <td>15.2 A more complicated but realistic scenario</td>
      <td>205</td>
    </tr>
    <tr>
      <td>15.3 Matching estimator for the average causal effect</td>
      <td>206</td>
    </tr>
    <tr>
      <td>15.3.1 Point estimation and bias correction</td>
      <td>206</td>
    </tr>
    <tr>
      <td>15.3.2 Connection with the doubly robust estimators</td>
      <td>208</td>
    </tr>
    <tr>
      <td>15.4 Matching estimator for the average causal effect on the treated</td>
      <td>209</td>
    </tr>
    <tr>
      <td>15.5 A case study</td>
      <td>210</td>
    </tr>
    <tr>
      <td>15.5.1 Experimental data</td>
      <td>210</td>
    </tr>
    <tr>
      <td>15.5.2 Observational data</td>
      <td>212</td>
    </tr>
    <tr>
      <td>15.5.3 Covariate balance checks</td>
      <td>213</td>
    </tr>
    <tr>
      <td>15.6 Discussion</td>
      <td>214</td>
    </tr>
    <tr>
      <td>15.7 Homework Problems</td>
      <td>214</td>
    </tr>
  </tbody>
</table>

IV Difficulties and challenges of observational studies

217<table>
  <tbody>
    <tr>
      <td>16</td>
      <td>Difficulties of Unconfoundedness in Observational Studies for Causal Effects</td>
      <td>219</td>
    </tr>
    <tr>
      <td>16.1</td>
      <td>Some basics of the causal diagram</td>
      <td>219</td>
    </tr>
    <tr>
      <td>16.2</td>
      <td>Assessing the unconfoundedness assumption</td>
      <td>220</td>
    </tr>
    <tr>
      <td>16.2.1</td>
      <td>Using negative outcomes</td>
      <td>221</td>
    </tr>
    <tr>
      <td>16.2.2</td>
      <td>Using negative exposures</td>
      <td>222</td>
    </tr>
    <tr>
      <td>16.2.3</td>
      <td>Summary</td>
      <td>222</td>
    </tr>
    <tr>
      <td>16.3</td>
      <td>Problems of over-adjustment</td>
      <td>223</td>
    </tr>
    <tr>
      <td>16.3.1</td>
      <td>M-bias</td>
      <td>223</td>
    </tr>
    <tr>
      <td>16.3.2</td>
      <td>Z-bias</td>
      <td>225</td>
    </tr>
    <tr>
      <td>16.3.3</td>
      <td>What covariates should we adjust for in observational studies?</td>
      <td>227</td>
    </tr>
    <tr>
      <td>16.4</td>
      <td>Homework Problems</td>
      <td>228</td>
    </tr>
    <tr>
      <td>17</td>
      <td>E-Value: Evidence for Causation in Observational Studies with Unmeasured Confounding</td>
      <td>231</td>
    </tr>
    <tr>
      <td>17.1</td>
      <td>Cornfield-type sensitivity analysis</td>
      <td>231</td>
    </tr>
    <tr>
      <td>17.2</td>
      <td>E-value</td>
      <td>235</td>
    </tr>
    <tr>
      <td>17.3</td>
      <td>A classic example</td>
      <td>236</td>
    </tr>
    <tr>
      <td>17.4</td>
      <td>Extensions</td>
      <td>237</td>
    </tr>
    <tr>
      <td>17.4.1</td>
      <td>E-value and Bradford Hill's criteria for causation</td>
      <td>237</td>
    </tr>
    <tr>
      <td>17.4.2</td>
      <td>E-value after logistic regression</td>
      <td>239</td>
    </tr>
    <tr>
      <td>17.4.3</td>
      <td>Non-zero true causal effect</td>
      <td>241</td>
    </tr>
    <tr>
      <td>17.5</td>
      <td>Critiques and responses</td>
      <td>241</td>
    </tr>
    <tr>
      <td>17.5.1</td>
      <td>E-value is just a monotone transformation of the risk ratio</td>
      <td>241</td>
    </tr>
    <tr>
      <td>17.5.2</td>
      <td>Calibration of the E-value</td>
      <td>242</td>
    </tr>
    <tr>
      <td>17.5.3</td>
      <td>It works the best for a binary outcome and the risk ratio</td>
      <td>242</td>
    </tr>
    <tr>
      <td>17.6</td>
      <td>Homework Problems</td>
      <td>243</td>
    </tr>
    <tr>
      <td>18</td>
      <td>Sensitivity Analysis for the Average Causal Effect with Unmeasured Confounding</td>
      <td>247</td>
    </tr>
    <tr>
      <td>18.1</td>
      <td>Introduction</td>
      <td>247</td>
    </tr>
    <tr>
      <td>18.2</td>
      <td>Manski-type worse-case bounds on the average causal effect without assumptions</td>
      <td>248</td>
    </tr>
    <tr>
      <td>18.3</td>
      <td>Sensitivity analysis for the average causal effect</td>
      <td>250</td>
    </tr>
    <tr>
      <td>18.3.1</td>
      <td>Identification formulas</td>
      <td>250</td>
    </tr>
    <tr>
      <td>18.3.2</td>
      <td>Example</td>
      <td>252</td>
    </tr>
    <tr>
      <td>18.3.2.1</td>
      <td>R functions for sensitivity analysis</td>
      <td>252</td>
    </tr>
    <tr>
      <td>18.3.2.2</td>
      <td>Revisit Example 10.3</td>
      <td>253</td>
    </tr>
    <tr>
      <td>18.4</td>
      <td>Homework Problems</td>
      <td>254</td>
    </tr>
  </tbody>
</table><table>
  <tbody>
    <tr>
      <td>19</td>
      <td>Rosenbaum-Style p-Values for Matched Observational Studies with Unmeasured Confounding</td>
      <td>257</td>
    </tr>
    <tr>
      <td>19.1</td>
      <td>A model for sensitivity analysis with matched data</td>
      <td>257</td>
    </tr>
    <tr>
      <td>19.2</td>
      <td>Worst-case p-values under Rosenbaum's sensitivity analysis model</td>
      <td>258</td>
    </tr>
    <tr>
      <td>19.3</td>
      <td>Examples</td>
      <td>259</td>
    </tr>
    <tr>
      <td>19.3.1</td>
      <td>Revisiting the LaLonde data</td>
      <td>259</td>
    </tr>
    <tr>
      <td>19.3.2</td>
      <td>Two examples from Rosenbaum's packages</td>
      <td>260</td>
    </tr>
    <tr>
      <td>19.4</td>
      <td>Homework Problems</td>
      <td>263</td>
    </tr>
    <tr>
      <td>20</td>
      <td>Overlap in Observational Studies: Difficulties and Opportunities</td>
      <td>267</td>
    </tr>
    <tr>
      <td>20.1</td>
      <td>Implications of overlap</td>
      <td>267</td>
    </tr>
    <tr>
      <td>20.1.1</td>
      <td>Trimming in the presence of limited overlap</td>
      <td>268</td>
    </tr>
    <tr>
      <td>20.1.2</td>
      <td>Outcome modeling in the presence of limited overlap</td>
      <td>268</td>
    </tr>
    <tr>
      <td>20.2</td>
      <td>Causal inference with no overlap: regression discontinuity</td>
      <td>269</td>
    </tr>
    <tr>
      <td>20.2.1</td>
      <td>Examples and graphical diagnostics</td>
      <td>269</td>
    </tr>
    <tr>
      <td>20.2.2</td>
      <td>A mathematical formulation of regression discontinuity</td>
      <td>271</td>
    </tr>
    <tr>
      <td>20.2.3</td>
      <td>Regressions near the boundary</td>
      <td>272</td>
    </tr>
    <tr>
      <td>20.2.4</td>
      <td>An example</td>
      <td>274</td>
    </tr>
    <tr>
      <td>20.2.5</td>
      <td>Problems of regression discontinuity</td>
      <td>276</td>
    </tr>
    <tr>
      <td>20.3</td>
      <td>Homework Problems</td>
      <td>277</td>
    </tr>
    <tr>
      <td>V</td>
      <td>Instrumental variables</td>
      <td>279</td>
    </tr>
    <tr>
      <td>21</td>
      <td>An Experimental Perspective of the Instrumental Variable</td>
      <td>281</td>
    </tr>
    <tr>
      <td>21.1</td>
      <td>Encouragement Design and Noncompliance</td>
      <td>281</td>
    </tr>
    <tr>
      <td>21.2</td>
      <td>Latent Compliance Status and Effects</td>
      <td>282</td>
    </tr>
    <tr>
      <td>21.2.1</td>
      <td>Nonparametric identification</td>
      <td>282</td>
    </tr>
    <tr>
      <td>21.2.2</td>
      <td>Estimation</td>
      <td>285</td>
    </tr>
    <tr>
      <td>21.3</td>
      <td>Covariates</td>
      <td>287</td>
    </tr>
    <tr>
      <td>21.3.1</td>
      <td>Covariate adjustment in the CRE</td>
      <td>287</td>
    </tr>
    <tr>
      <td>21.3.2</td>
      <td>Covariates in conditional randomization or uncon- founded observational studies</td>
      <td>287</td>
    </tr>
    <tr>
      <td>21.4</td>
      <td>Weak IV</td>
      <td>288</td>
    </tr>
    <tr>
      <td>21.4.1</td>
      <td>Some simulation</td>
      <td>288</td>
    </tr>
    <tr>
      <td>21.4.2</td>
      <td>A procedure robust to weak IV</td>
      <td>288</td>
    </tr>
    <tr>
      <td>21.4.3</td>
      <td>Implementation and simulation</td>
      <td>291</td>
    </tr>
    <tr>
      <td>21.5</td>
      <td>Application</td>
      <td>293</td>
    </tr>
    <tr>
      <td>21.6</td>
      <td>Interpreting the CACE</td>
      <td>294</td>
    </tr>
    <tr>
      <td>21.7</td>
      <td>Homework problems</td>
      <td>296</td>
    </tr>
  </tbody>
</table><table>
  <tbody>
    <tr>
      <td><b>22 Disentangle Mixture Distributions and Instrumental Variable Inequalities</b></td>
      <td><b>301</b></td>
    </tr>
    <tr>
      <td>22.1 Disentangle Mixture Distributions</td>
      <td>301</td>
    </tr>
    <tr>
      <td>22.2 Testable implications: Instrumental Variable Inequalities</td>
      <td>304</td>
    </tr>
    <tr>
      <td>22.3 Examples</td>
      <td>304</td>
    </tr>
    <tr>
      <td>22.4 Homework problems</td>
      <td>308</td>
    </tr>
    <tr>
      <td><b>23 An Econometric Perspective of the Instrumental Variable</b></td>
      <td><b>315</b></td>
    </tr>
    <tr>
      <td>23.1 Examples of studies with IVs</td>
      <td>316</td>
    </tr>
    <tr>
      <td>23.2 Brief Review of the Ordinary Least Squares</td>
      <td>317</td>
    </tr>
    <tr>
      <td>23.3 Linear Instrumental Variable Model</td>
      <td>319</td>
    </tr>
    <tr>
      <td>23.4 The Just-Identified Case</td>
      <td>320</td>
    </tr>
    <tr>
      <td>23.5 The Over-Identified Case</td>
      <td>321</td>
    </tr>
    <tr>
      <td>23.6 A Special Case: A Single IV for a Single Endogenous Treatment</td>
      <td>323</td>
    </tr>
    <tr>
      <td>23.6.1 Two-stage least squares</td>
      <td>323</td>
    </tr>
    <tr>
      <td>23.6.2 Indirect least squares</td>
      <td>323</td>
    </tr>
    <tr>
      <td>23.6.3 Weak IV</td>
      <td>324</td>
    </tr>
    <tr>
      <td>23.7 Application</td>
      <td>325</td>
    </tr>
    <tr>
      <td>23.8 Homework</td>
      <td>326</td>
    </tr>
    <tr>
      <td><b>24 Application of the Instrumental Variable Method: Fuzzy Regression Discontinuity</b></td>
      <td><b>329</b></td>
    </tr>
    <tr>
      <td>24.1 Motivating examples</td>
      <td>329</td>
    </tr>
    <tr>
      <td>24.2 Mathematical formulation</td>
      <td>330</td>
    </tr>
    <tr>
      <td>24.3 Application</td>
      <td>332</td>
    </tr>
    <tr>
      <td>24.3.1 Re-analyzing Asher and Novosad (2020)'s data</td>
      <td>332</td>
    </tr>
    <tr>
      <td>24.3.2 Re-analyzing Li et al. (2015)'s data</td>
      <td>333</td>
    </tr>
    <tr>
      <td>24.4 Discussion</td>
      <td>335</td>
    </tr>
    <tr>
      <td>24.5 Homework Problems</td>
      <td>337</td>
    </tr>
    <tr>
      <td><b>25 Application of the Instrumental Variable Method: Mendelian Randomization</b></td>
      <td><b>339</b></td>
    </tr>
    <tr>
      <td>25.1 Background and motivation</td>
      <td>339</td>
    </tr>
    <tr>
      <td>25.2 MR based on summary statistics</td>
      <td>342</td>
    </tr>
    <tr>
      <td>25.2.1 Fixed-effect estimator</td>
      <td>342</td>
    </tr>
    <tr>
      <td>25.2.2 Egger regression</td>
      <td>343</td>
    </tr>
    <tr>
      <td>25.3 An example</td>
      <td>344</td>
    </tr>
    <tr>
      <td>25.4 Critiques of the analysis based on MR</td>
      <td>346</td>
    </tr>
    <tr>
      <td>25.5 Homework Problems</td>
      <td>346</td>
    </tr>
    <tr>
      <td><b>VI Causal Mechanisms with Post-Treatment Variables</b></td>
      <td><b>347</b></td>
    </tr>
  </tbody>
</table><table>
  <tbody>
    <tr>
      <td><i>Contents</i></td>
      <td style="text-align: right;">xiii</td>
    </tr>
    <tr>
      <td><b>26 Principal Stratification</b></td>
      <td style="text-align: right;"><b>349</b></td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">26.1 Motivating Examples</td>
      <td style="text-align: right;">349</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">26.2 The Problem of Conditioning on the Post-Treatment Variable</td>
      <td style="text-align: right;">350</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">26.3 Conditioning on the Potential Values of the Post-Treatment Variable</td>
      <td style="text-align: right;">352</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">26.4 Statistical Inference and Its Difficulty</td>
      <td style="text-align: right;">353</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">26.4.1 Special case: truncation by death with binary outcome</td>
      <td style="text-align: right;">354</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">26.4.2 An application</td>
      <td style="text-align: right;">356</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">26.4.3 Extensions</td>
      <td style="text-align: right;">356</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">26.5 Principal score method</td>
      <td style="text-align: right;">357</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">26.5.1 Principal score method under strong monotonicity</td>
      <td style="text-align: right;">357</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">26.5.2 An example</td>
      <td style="text-align: right;">359</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">26.5.3 Extensions</td>
      <td style="text-align: right;">360</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">26.6 Other methods</td>
      <td style="text-align: right;">360</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">26.7 Homework problems</td>
      <td style="text-align: right;">361</td>
    </tr>
    <tr>
      <td><b>27 Mediation Analysis: Natural Direct and Indirect Effects</b></td>
      <td style="text-align: right;"><b>365</b></td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">27.1 Motivating Examples</td>
      <td style="text-align: right;">365</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">27.2 Nested Potential Outcomes</td>
      <td style="text-align: right;">366</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">27.2.1 Natural Direct and Indirect Effects</td>
      <td style="text-align: right;">366</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">27.2.2 Metaphysics or Science</td>
      <td style="text-align: right;">368</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">27.3 The Mediation Formula</td>
      <td style="text-align: right;">370</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">27.4 The Mediation Formula Under Linear Models</td>
      <td style="text-align: right;">374</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">27.4.1 The Baron-Kenny Method</td>
      <td style="text-align: right;">374</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">27.4.2 An Example</td>
      <td style="text-align: right;">375</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">27.5 Sensitivity analysis</td>
      <td style="text-align: right;">377</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">27.6 Homework problems</td>
      <td style="text-align: right;">377</td>
    </tr>
    <tr>
      <td><b>28 Controlled Direct Effect</b></td>
      <td style="text-align: right;"><b>381</b></td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">28.1 Definition of the controlled direct effect</td>
      <td style="text-align: right;">381</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">28.2 Identification and estimation of the controlled direct effect</td>
      <td style="text-align: right;">381</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">28.3 Discussion</td>
      <td style="text-align: right;">383</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">28.4 Homework problems</td>
      <td style="text-align: right;">384</td>
    </tr>
    <tr>
      <td><b>29 Time-Varying Treatment and Confounding</b></td>
      <td style="text-align: right;"><b>387</b></td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">29.1 Basic setup and sequential ignorability</td>
      <td style="text-align: right;">387</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">29.2 g-formula and outcome modeling</td>
      <td style="text-align: right;">389</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">29.2.1 Plug-in estimation based on outcome modeling</td>
      <td style="text-align: right;">390</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">29.2.2 Recursive estimation based on outcome modeling</td>
      <td style="text-align: right;">392</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">29.3 Inverse propensity score weighting</td>
      <td style="text-align: right;">393</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">29.4 Multiple time points</td>
      <td style="text-align: right;">394</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">29.4.1 Marginal structural model</td>
      <td style="text-align: right;">394</td>
    </tr>
    <tr>
      <td style="padding-left: 4em;">29.4.2 Structural nested model</td>
      <td style="text-align: right;">395</td>
    </tr>
    <tr>
      <td style="padding-left: 2em;">29.5 Homework problems</td>
      <td style="text-align: right;">398</td>
    </tr>
    <tr>
      <td><b>VII Appendices</b></td>
      <td style="text-align: right;"><b>405</b></td>
    </tr>
  </tbody>
</table><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td>A</td><td>Probability and Statistics</td><td>407</td></tr><tr><td>A.1</td><td>Probability</td><td>407</td></tr><tr><td>A.1.1</td><td>Pearson correlation coefficient and squared multiple correlation coefficient</td><td>407</td></tr><tr><td>A.1.2</td><td>Multivariate Normal random vector</td><td>407</td></tr><tr><td>A.1.3</td><td>χ² and t distributions</td><td>408</td></tr><tr><td>A.1.4</td><td>Cauchy-Schwarz inequality</td><td>408</td></tr><tr><td>A.1.5</td><td>Tower property and variance decomposition</td><td>409</td></tr><tr><td>A.1.6</td><td>Limiting theorems</td><td>409</td></tr><tr><td>A.1.7</td><td>Delta method</td><td>410</td></tr><tr><td>A.2</td><td>Statistical inference</td><td>411</td></tr><tr><td>A.2.1</td><td>Point estimation</td><td>411</td></tr><tr><td>A.2.2</td><td>Confidence interval</td><td>412</td></tr><tr><td>A.2.3</td><td>Hypothesis testing</td><td>412</td></tr><tr><td>A.2.4</td><td>Wald-type confidence interval and test</td><td>413</td></tr><tr><td>A.2.5</td><td>Duality between constructing confidence sets and testing null hypotheses</td><td>413</td></tr><tr><td>A.3</td><td>Inference with two-by-two tables</td><td>414</td></tr><tr><td>A.3.1</td><td>Fisher's exact test</td><td>414</td></tr><tr><td>A.3.2</td><td>Estimation with two-by-two tables</td><td>415</td></tr><tr><td>A.4</td><td>Two famous problems in statistics</td><td>415</td></tr><tr><td>A.4.1</td><td>Behrens-Fisher problem</td><td>415</td></tr><tr><td>A.4.2</td><td>Fieller-Creasy problem</td><td>416</td></tr><tr><td>A.5</td><td>Monte Carlo method in statistics</td><td>417</td></tr><tr><td>A.6</td><td>Bootstrap</td><td>417</td></tr><tr><td>A.7</td><td>Homework problems</td><td>419</td></tr><tr><td>B</td><td>Linear and Logistic Regressions</td><td>421</td></tr><tr><td>B.1</td><td>Population ordinary least squares</td><td>421</td></tr><tr><td>B.2</td><td>Sample ordinary least squares</td><td>422</td></tr><tr><td>B.3</td><td>Frisch-Waugh-Lovell Theorem</td><td>423</td></tr><tr><td>B.4</td><td>Linear model</td><td>424</td></tr><tr><td>B.5</td><td>Weighted least squares</td><td>426</td></tr><tr><td>B.6</td><td>Logistic regression</td><td>427</td></tr><tr><td>B.6.1</td><td>Model</td><td>427</td></tr><tr><td>B.6.2</td><td>Maximum likelihood estimate</td><td>428</td></tr><tr><td>B.6.3</td><td>Extension to the case-control study</td><td>429</td></tr><tr><td>B.6.4</td><td>Logistic regression with weights</td><td>430</td></tr><tr><td>B.7</td><td>Homework problems</td><td>430</td></tr><tr><td>C</td><td>Some Useful Lemmas for Simple Random Sampling From a Finite Population</td><td>433</td></tr><tr><td>C.1</td><td>Lemmas</td><td>433</td></tr><tr><td>C.2</td><td>Proofs</td><td>435</td></tr><tr><td>C.3</td><td>Comments on the literature</td><td>437</td></tr></tbody></table><table><tr><td><em>Contents</em></td><td>xv</td></tr><tr><td>C.4 Homework Problems</td><td>438</td></tr></table>![[_resources/Ding - 2023 - A First Course in Causal Inference_Part1/b88bf78076edf6a34d5c6910d41905ff_MD5.jpg]]

![[_resources/Ding - 2023 - A First Course in Causal Inference_Part1/543ac8a91f7faea799c7fa9e33d2f612_MD5.jpg]]Preface

Causal inference research and education in the past decade

The past decade has witnessed an explosion of interest in research and edu-
cation in causal inference, due to its wide applications in biomedical research,
social sciences, tech companies, etc. It was quite different even ten years ago
when I was a Ph.D. student in statistics. At that time, causal inference was
not a mainstream research topic in statistics and very few undergraduate and
graduate programs offered courses in causal inference. In the academic world
of statistics, many people were still very skeptical about the foundation of
causal inference. Many leading statisticians were reluctant to accept causal
inference because of the fundamental conceptual difficulties, which differ from
the traditional training of mathematical statistics.

The applications of causal inference in empirical research have changed
the field of statistics in both research and education. In the end, statistics is
not only about abstract theory but also about solving real-world problems.
Many talented researchers have joined the effort to advance our knowledge of
causal inference. Many students are eager to learn state-of-the-art theory and
methods in causal inference so that they are better equipped to solve problems
from various fields.

Due to the needs of the students, my colleagues encouraged me to develop
a course in causal inference. Initially, I taught a graduate-level course cross-
listed under Political Science and Statistics, which was taught by my former
colleague Jas Sekhon for many years at UC Berkeley. Later, I developed this
course for both undergraduate and graduate students. At UC Berkeley, the
course numbers for “Causal Inference” are Stat 156 and Stat 256, with un-
dergraduate students in Stat 156 and graduate students in Stat 256. Students
in both sessions used the same lecture notes and attended the same lectures
given by me and my teaching assistants, although they needed to finish dif-
ferent homework problems, reading assignments, and final projects.

Given the mixed levels of technical preparations of my students, the most
challenging part of my teaching was to balance the interests of both under-
graduate and graduate students. On the one hand, I wanted to present the
materials in an intuitive way and only required the undergraduate students
to have the basic knowledge of probability, statistics, linear regression, and
logistic regression. On the other hand, I also wanted to introduce recent re-
search topics and results to the graduate students. This book is a product of
my efforts in the past seven years.## Recommendations for instructors

This book contains 29 chapters in the main text and 3 chapters in the appendix. UC Berkeley is on the semester system and each semester has 14 weeks of lectures. I could not finish all 32 chapters in one semester. Here are some recommendations based on my own teaching experience.

### Appendix

I started with the chapters in the main text but asked my teaching assistants to review the basics in Chapters A and B. To encourage the students to review Chapters A–C before reading the main text, I also assigned several homework problems from Chapters A–C at the beginning of the semester.

### Part I

The key topic in Chapter 1 is the Yule–Simpson Paradox. Chapter 2 introduces the notion of potential outcomes, which is the foundation for the whole book.

### Part II

Different researchers and instructors may have quite different views on the materials in Part II on randomized experiments. I have talked to many friends about the pros and cons of the current presentation in Part II. Causal inference in randomized experiments is relatively straightforward because randomization eliminates unmeasured confounding. So some friends feel that Chapters 3–9 are too long at least for beginners of causal inference. This also disappointed some of my students when I spent a month on randomized experiments. On the other hand, I was trained from the book of Imbens and Rubin (2015) and believed that to understand observational studies, it is better to understand randomized experiments first. Moreover, I am a big fan of the canonical research of Neyman (1923) and Fisher (1935). Therefore, Part II deeply reflects my own intellectual history and personal taste in statistics. Other instructors may not want to spend a month on randomized experiments and can cover Chapters 5, 7, 8 and 9 quickly.

### Part III

Part III covers the key ideas in observational studies without unmeasured confounding. Four pillars of observational studies are outcome regression, inverse propensity score weighting, doubly robust, and matching estimators, which are covered in Chapters 10, 11, 12 and 15, respectively. Chapters 13 and 14 are optional in teaching. But the results in Chapters 13 and 14 are not uninteresting, so I sometimes covered one or two results there, asked the teaching assistants to cover more in the lab sessions, and encouraged students to read them by assigning some homework problems from those chapters.Preface xix

Part IV

Part IV is a novel treatment of the fundamental difficulties of observational
studies including unmeasured confounding and overlap. However, this part is
far from perfect due to the complexities and subtleties of the issues. Chapters
17, 18 and 20 are central, whereas Chapters 16 and 19 are optional.

Part V

Part V discusses the idea of the instrumental variable. Chapters 21, 23 and 24 are key, whereas Chapters 22 and 25 are optional.

Part VI

Part VI are some special topics. They are all optional in some sense. Probably
it is worth teaching Chapter 27 given the popularity of the Baron–Kenny
method in mediation analysis.

Omitted topics

This book does not cover some popular econometric methods including the
difference in differences, panel data, and synthetic controls. Instructors can
use Angrist and Pischke (2008) as a reference for those topics. This book
assumes minimal preparation for the background knowledge in probability
and statistics. Since most introductory statistics courses use the frequentists'
view that assumes the unknown parameters are fixed, I adopt this view in
this book and omit the Bayesian view for causal inference. In fact, many
fundamental ideas of causal inference are from the Bayesian view, starting
from Rubin (1978). If readers and students are interested in Bayesian causal
inference, please read the review paper by Li et al. (2023).

Help from teaching assistants

My teaching assistants offered invaluable help for my courses at UC Berkeley.
Since I could not cover everything in my notes, I consistently relied on them
to cover some technical details or R program sessions in their labs.

Solution to some homework problems

I have also prepared the solutions to most theory problems. If you are an
instructor for a causal inference course, please contact me for the solutions
with detailed information about your course.

**Additional recommendations for readers and students**

Readers and students can first read my recommendations for instructors
above. In addition, I have two other recommendations.Preface

## *Homework problems*

Each chapter of this book contains homework problems. To deepen the understanding, it is important to try some homework problems. Moreover, some homework problems contain useful theoretical results. Even if you do not have time to figure out the details for those problems, it is helpful to at least read the statements of the problems.

## *Recommended reading*

Each chapter of this book contains recommended reading. If you want to do research in causal inference, those recommended papers can be useful background knowledge of the literature. When I taught the graduate-level causal inference course at UC Berkeley, I assigned the following papers to the students as weekly reading from week one to the end of the semester:

* Bickel et al. (1975);
* Holland (1986);
* Miratrix et al. (2013);
* Lin (2013);
* Li et al. (2018b);
* Rosenbaum and Rubin (1983b);
* Lunceford and Davidian (2004);
* Ding and VanderWeele (2016);
* Pearl (1995);
* Angrist et al. (1996);
* Imbens (2014);
* Frangakis and Rubin (2002).

Many students gave me positive feedback about their experience of reading the papers above. I recommend reading the above papers even if you do not read this book.

## Features of the book

There are already many excellent causal inference books published in the last decade. Some of them have profound influences on me. When I was in college, I read some draft chapters of Imbens and Rubin (2015) from the internet. They completely challenged my way of thinking about statistics and helped to build my research interest in causal inference. I read Angrist and Pischke (2008)many times and have gained new insights each time I re-read it. Rosenbaum (2002b), Morgan and Winship (2015), and Hernán and Robins (2020) are another three excellent books from leading researchers in causal inference. When I was preparing for the book, Cunningham (2021), Huntington-Klein (2022), Brumback (2022) and Huber (2023) appeared as four recent excellent books on causal inference.

Thanks to my teaching experience at UC Berkeley, this book has the fol-
lowing features that instructors, students, and readers may find attractive.

* This book assumes minimal preparation for causal inference and reviews the basic probability and statistics knowledge in the appendix.
* This book covers causal inference from the statistics, biostatistics, and econometrics perspectives, and draws applications from various fields.
* This book uses R code and data analysis to illustrate the ideas of causal inference. All the R code and datasets are publicly available at Harvard Dataverse: https://doi.org/10.7910/DVN/ZX3VEV
* This book contains homework problems and can be used as a textbook for both undergraduate and graduate students. Instructors can also ask me for solutions to some homework problems.

Acknowledgments

Professor Zhi Geng at Peking University introduced me to the area of causal
inference when I was studying in college. Professors Luke Miratrix, Tirthankar
Dasgupta, and Don Rubin served on my Ph.D. thesis committee at the Har-
vard Statistics Department. Professor Tyler VanderWeele supervised me as a
postdoctoral researcher in Epidemiology at the Harvard T. H. Chan School of
Public Health.

My colleagues at the Berkeley Statistics Department have created a critical
and productive research environment. Bin Yu and Jas Sekhon have been very
supportive since I was a junior faculty. My department chairs, Deb Nolan,
Sandrine Dudoit, and Haiyan Huang, encouraged me to develop the “Causal
Inference” course. It has been a rewarding experience for me.

I have been lucky to work with many collaborators, in particular, Avi
Feller, Laura Forastiere, Zhichao Jiang, Fang Han, Fan Li, Xinran Li, Alessan-
dra Mattei, Fabrizia Mealli, Shu Yang, and Anqi Zhao. I will report in this
book what I have learned from them.

Many students at UC Berkeley made critical and constructive comments
on early versions of my lecture notes. As teaching assistants for my “Causal
Inference” course, Emily Flanagan and Sizhu Lu read early versions of my
book carefully and helped me to improve the book a lot.

Professor Joe Blitzstein read an early version of the book carefully and
made very detailed comments. Addressing his comments leads to significantPreface

improvement in the book. Professors Hongyuan Cao and Zhichao Jiang taught
“Causal Inference” courses based on an early version of the book. They made
very valuable suggestions.

I am also very grateful for the suggestions from Young Woong Min,
Fangzhou Su, Chaoran Yu, and Lo-Hua Yuan.

If you identify any errors, please feel free to email me.